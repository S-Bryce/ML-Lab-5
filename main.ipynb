{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86c4bda",
   "metadata": {},
   "source": [
    "# Python Group\n",
    "## Lab Assignment Five: Wide and Deep Network Architectures\n",
    "### Wali Chaudhary, Bryce Shurts, & Alex Wright"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af775570",
   "metadata": {},
   "source": [
    "#### Citation & Acknowledgement\n",
    "\n",
    "Placeholder text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc9357",
   "metadata": {},
   "source": [
    "### Preparation (4 points total)\n",
    "    \n",
    "    [1 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). \n",
    "\t\n",
    "    [1 points] Identify groups of features in your data that should be combined into cross-product features. Provide justification for why these features should be crossed (or why some features should not be crossed). \n",
    "\t\n",
    "    [1 points] Choose and explain what metric(s) you will use to evaluate your algorithm’s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n",
    "\t\n",
    "    [1 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Argue why your cross validation method is a realistic mirroring of how an algorithm would be used in practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a6fd54",
   "metadata": {},
   "source": [
    "###\tModeling (5 points total)\n",
    "\t[2 points] Create at least three combined wide and deep networks to classify your data using Keras. Visualize the performance of the network on the training data and validation data in the same plot versus the training iterations. Note: use the \"history\" return parameter that is part of Keras \"fit\" function to easily access this data.\n",
    "\t\n",
    "    [2 points] Investigate generalization performance by altering the number of layers in the deep branch of the network. Try at least two different number of layers. Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab to select the number of layers that performs superiorly. \n",
    "\t\n",
    "    [1 points] Compare the performance of your best wide and deep network to a standard multi-layer perceptron (MLP). Alternatively, you can compare to a network without the wide branch (i.e., just the deep network). .  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed98d95",
   "metadata": {},
   "source": [
    "###\tExceptional Work (1 points total) \n",
    "\n",
    "    (0.4) The overall quality of the report as a coherent, useful, complete and polished product will be reflected here. Did you support your reasons for creating cross categories? Did you include plots and provide interpretation? Did you justify your choice of metrics?\n",
    "\n",
    "    (0.6) Additional analysis\n",
    "\n",
    "    (5000 level) You have free rein to provide any additional analyses and visualizations appropriate to your data.  Implement additional optimization models or statistical analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd3d7c6",
   "metadata": {},
   "source": [
    "###\t( 7000 level students - options): \n",
    "\n",
    "    Capture the embedding weights from the deep network and (if needed) perform dimensionality reduction on the output of these embedding layers (only if needed). That is, pass the observations into the network, save the embedded weights (called embeddings), and then perform  dimensionality reduction in order to visualize results. Visualize and explain any clusters in the data.\n",
    "\n",
    "    Use statistical methods to compare the performance of different models.  For classification tasks, you ight compare using the receiver operating characteristic and area under the curve. For regression tasks, you might use use Bland-Altman plots and residual variance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c3b6fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.12.0\n",
      "Keras version: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "# Handle all imports for notebook\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics as mt\n",
    "import pprint\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import concatenate\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "203975af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  class cap-shape cap-surface cap-color bruises odor gill-attachment  \\\n0     p         x           s         n       t    p               f   \n1     e         x           s         y       t    a               f   \n2     e         b           s         w       t    l               f   \n3     p         x           y         w       t    p               f   \n4     e         x           s         g       f    n               f   \n\n  gill-spacing gill-size gill-color  ... stalk-surface-below-ring  \\\n0            c         n          k  ...                        s   \n1            c         b          k  ...                        s   \n2            c         b          n  ...                        s   \n3            c         n          n  ...                        s   \n4            w         b          k  ...                        s   \n\n  stalk-color-above-ring stalk-color-below-ring veil-type veil-color  \\\n0                      w                      w         p          w   \n1                      w                      w         p          w   \n2                      w                      w         p          w   \n3                      w                      w         p          w   \n4                      w                      w         p          w   \n\n  ring-number ring-type spore-print-color population habitat  \n0           o         p                 k          s       u  \n1           o         p                 n          n       g  \n2           o         p                 n          n       m  \n3           o         p                 k          s       u  \n4           o         e                 n          a       g  \n\n[5 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>cap-shape</th>\n      <th>cap-surface</th>\n      <th>cap-color</th>\n      <th>bruises</th>\n      <th>odor</th>\n      <th>gill-attachment</th>\n      <th>gill-spacing</th>\n      <th>gill-size</th>\n      <th>gill-color</th>\n      <th>...</th>\n      <th>stalk-surface-below-ring</th>\n      <th>stalk-color-above-ring</th>\n      <th>stalk-color-below-ring</th>\n      <th>veil-type</th>\n      <th>veil-color</th>\n      <th>ring-number</th>\n      <th>ring-type</th>\n      <th>spore-print-color</th>\n      <th>population</th>\n      <th>habitat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>p</td>\n      <td>x</td>\n      <td>s</td>\n      <td>n</td>\n      <td>t</td>\n      <td>p</td>\n      <td>f</td>\n      <td>c</td>\n      <td>n</td>\n      <td>k</td>\n      <td>...</td>\n      <td>s</td>\n      <td>w</td>\n      <td>w</td>\n      <td>p</td>\n      <td>w</td>\n      <td>o</td>\n      <td>p</td>\n      <td>k</td>\n      <td>s</td>\n      <td>u</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>e</td>\n      <td>x</td>\n      <td>s</td>\n      <td>y</td>\n      <td>t</td>\n      <td>a</td>\n      <td>f</td>\n      <td>c</td>\n      <td>b</td>\n      <td>k</td>\n      <td>...</td>\n      <td>s</td>\n      <td>w</td>\n      <td>w</td>\n      <td>p</td>\n      <td>w</td>\n      <td>o</td>\n      <td>p</td>\n      <td>n</td>\n      <td>n</td>\n      <td>g</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>e</td>\n      <td>b</td>\n      <td>s</td>\n      <td>w</td>\n      <td>t</td>\n      <td>l</td>\n      <td>f</td>\n      <td>c</td>\n      <td>b</td>\n      <td>n</td>\n      <td>...</td>\n      <td>s</td>\n      <td>w</td>\n      <td>w</td>\n      <td>p</td>\n      <td>w</td>\n      <td>o</td>\n      <td>p</td>\n      <td>n</td>\n      <td>n</td>\n      <td>m</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>p</td>\n      <td>x</td>\n      <td>y</td>\n      <td>w</td>\n      <td>t</td>\n      <td>p</td>\n      <td>f</td>\n      <td>c</td>\n      <td>n</td>\n      <td>n</td>\n      <td>...</td>\n      <td>s</td>\n      <td>w</td>\n      <td>w</td>\n      <td>p</td>\n      <td>w</td>\n      <td>o</td>\n      <td>p</td>\n      <td>k</td>\n      <td>s</td>\n      <td>u</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>e</td>\n      <td>x</td>\n      <td>s</td>\n      <td>g</td>\n      <td>f</td>\n      <td>n</td>\n      <td>f</td>\n      <td>w</td>\n      <td>b</td>\n      <td>k</td>\n      <td>...</td>\n      <td>s</td>\n      <td>w</td>\n      <td>w</td>\n      <td>p</td>\n      <td>w</td>\n      <td>o</td>\n      <td>e</td>\n      <td>n</td>\n      <td>a</td>\n      <td>g</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 23 columns</p>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df: DataFrame = pd.read_csv(\"mushrooms.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class                        2\n",
      "cap-shape                    6\n",
      "cap-surface                  4\n",
      "cap-color                   10\n",
      "bruises                      2\n",
      "odor                         9\n",
      "gill-attachment              2\n",
      "gill-spacing                 2\n",
      "gill-size                    2\n",
      "gill-color                  12\n",
      "stalk-shape                  2\n",
      "stalk-root                   5\n",
      "stalk-surface-above-ring     4\n",
      "stalk-surface-below-ring     4\n",
      "stalk-color-above-ring       9\n",
      "stalk-color-below-ring       9\n",
      "veil-type                    1\n",
      "veil-color                   4\n",
      "ring-number                  3\n",
      "ring-type                    5\n",
      "spore-print-color            9\n",
      "population                   6\n",
      "habitat                      7\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8124 entries, 0 to 8123\n",
      "Data columns (total 23 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   class                     8124 non-null   object\n",
      " 1   cap-shape                 8124 non-null   object\n",
      " 2   cap-surface               8124 non-null   object\n",
      " 3   cap-color                 8124 non-null   object\n",
      " 4   bruises                   8124 non-null   object\n",
      " 5   odor                      8124 non-null   object\n",
      " 6   gill-attachment           8124 non-null   object\n",
      " 7   gill-spacing              8124 non-null   object\n",
      " 8   gill-size                 8124 non-null   object\n",
      " 9   gill-color                8124 non-null   object\n",
      " 10  stalk-shape               8124 non-null   object\n",
      " 11  stalk-root                8124 non-null   object\n",
      " 12  stalk-surface-above-ring  8124 non-null   object\n",
      " 13  stalk-surface-below-ring  8124 non-null   object\n",
      " 14  stalk-color-above-ring    8124 non-null   object\n",
      " 15  stalk-color-below-ring    8124 non-null   object\n",
      " 16  veil-type                 8124 non-null   object\n",
      " 17  veil-color                8124 non-null   object\n",
      " 18  ring-number               8124 non-null   object\n",
      " 19  ring-type                 8124 non-null   object\n",
      " 20  spore-print-color         8124 non-null   object\n",
      " 21  population                8124 non-null   object\n",
      " 22  habitat                   8124 non-null   object\n",
      "dtypes: object(23)\n",
      "memory usage: 1.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.nunique())\n",
    "print(df.info())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, let's begin by defining the variable which classification will be based around, and describe how we will represent each one of our datasets' variables. Thankfully, all the variables/features are categorical, meaning that our encoding of the features is relatively straightforward (at least, on the face of things).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'class' will be represented as a dummy-encoded bool/int\n",
      "'cap-shape' will be represented as a binary-encoded integer\n",
      "'cap-surface' will be represented as a one-hot-encoded integer\n",
      "'cap-color' will be represented as a binary-encoded integer\n",
      "'bruises' will be represented as a dummy-encoded bool/int\n",
      "'odor' will be represented as a binary-encoded integer\n",
      "'gill-attachment' will be represented as a dummy-encoded bool/int\n",
      "'gill-spacing' will be represented as a dummy-encoded bool/int\n",
      "'gill-size' will be represented as a dummy-encoded bool/int\n",
      "'gill-color' will be represented as a binary-encoded integer\n",
      "'stalk-shape' will be represented as a dummy-encoded bool/int\n",
      "'stalk-root' will be represented as a one-hot-encoded integer\n",
      "'stalk-surface-above-ring' will be represented as a one-hot-encoded integer\n",
      "'stalk-surface-below-ring' will be represented as a one-hot-encoded integer\n",
      "'stalk-color-above-ring' will be represented as a binary-encoded integer\n",
      "'stalk-color-below-ring' will be represented as a binary-encoded integer\n",
      "'veil-type' will be dropped\n",
      "'veil-color' will be represented as a one-hot-encoded integer\n",
      "'ring-number' will be represented as a one-hot-encoded integer\n",
      "'ring-type' will be represented as a one-hot-encoded integer\n",
      "'spore-print-color' will be represented as a binary-encoded integer\n",
      "'population' will be represented as a binary-encoded integer\n",
      "'habitat' will be represented as a binary-encoded integer\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    num_values = df[column].nunique()\n",
    "    if num_values == 2:\n",
    "        print(\"'\" + column + \"' will be represented as a dummy-encoded bool/integer\")\n",
    "    elif 2 < num_values <= 5:\n",
    "        print(\"'\" + column + \"' will be represented as a one-hot-encoded integer\")\n",
    "    elif num_values > 5:\n",
    "        print(\"'\" + column + \"' will be represented as a binary-encoded integer\")\n",
    "    else: # 1 value\n",
    "        print(\"'\" + column + \"' will be dropped\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have a much better understanding of what we will be doing with our data. However, before we move forward with preprocessing work we will need to consider several salient factors. First and foremost, we want to avoid data leakage between our train and test sets, so anything that must be first fit/trained against to our data (e.g., the one-hot encoder) will need to take place after the dataset is split into a train and test set. Additionally, since our data is all categorical the encoding will explode our dimensionality: while we can combat it somewhat through binary encoding features that have a large number of possible values, we will still need to look at other methods like a kernel-based PCA for further reducing our dimensionality.\n",
    "\n",
    "We will also want to take a look at a correlation matrix between our classification target, \"class\", and the rest of our variables, as we do not want to waste processor cycles on irrelevant data: this can be done after the encoding work but before PCA, so that we can limit the number of input features that need to be analyzed.\n",
    "\n",
    "Therefore, we can begin with concerning the best way to split our data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing entries: False; 8124 samples total.\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing entries: \" + str(df.isna().sum().sum() > 0) + \"; \" + str(df.shape[0]) + \" samples total.\")\n",
    "if df.isna().sum().sum() > 0:\n",
    "    print(\"Removing samples with missing data...\")\n",
    "    df.dropna(inplace=True)\n",
    "    print(\"Missing entries: \" + str(df.isna().sum().sum() > 0) + \"; \" + str(df.shape[0]) + \" samples total.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99bfe78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 8125 entries, ('class', 'cap-shape', 'cap-surface') to ('e', 'x', 's')\n",
      "Data columns (total 20 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   class                     8125 non-null   object\n",
      " 1   cap-shape                 8125 non-null   object\n",
      " 2   cap-surface               8125 non-null   object\n",
      " 3   cap-color                 8125 non-null   object\n",
      " 4   bruises                   8125 non-null   object\n",
      " 5   odor                      8125 non-null   object\n",
      " 6   gill-attachment           8125 non-null   object\n",
      " 7   gill-spacing              8125 non-null   object\n",
      " 8   gill-size                 8125 non-null   object\n",
      " 9   gill-color                8125 non-null   object\n",
      " 10  stalk-surface-below-ring  8125 non-null   object\n",
      " 11  stalk-color-above-ring    8125 non-null   object\n",
      " 12  stalk-color-below-ring    8125 non-null   object\n",
      " 13  veil-type                 8125 non-null   object\n",
      " 14  veil-color                8125 non-null   object\n",
      " 15  ring-number               8125 non-null   object\n",
      " 16  ring-type                 8125 non-null   object\n",
      " 17  spore-print-color         8125 non-null   object\n",
      " 18  population                8125 non-null   object\n",
      " 19  habitat                   8125 non-null   object\n",
      "dtypes: object(20)\n",
      "memory usage: 1.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "headers = ['class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat']\n",
    "\n",
    "df_train_orig = pd.read_csv('mushrooms.csv', names=headers)\n",
    "df_test_orig = pd.read_csv('mushrooms.csv', names=headers)\n",
    "\n",
    "df_test_orig = df_test_orig.iloc[1:]\n",
    "print(df_train_orig.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf8a4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = deepcopy(df_train_orig)\n",
    "df_test = deepcopy(df_test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3039344",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# let's just get rid of rows with any missing data\n",
    "# and then reset the indices of the dataframe so it corresponds to row number\n",
    "df_train.replace(to_replace=' ?',value=np.nan, inplace=True)\n",
    "df_train.dropna(inplace=True)\n",
    "df_train.reset_index()\n",
    "\n",
    "df_test.replace(to_replace=' ?',value=np.nan, inplace=True)\n",
    "df_test.dropna(inplace=True)\n",
    "df_test.reset_index()\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e70a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# replace weird strings inconsistency\n",
    "if df_test.income.dtype=='object':\n",
    "    df_test.income.replace(to_replace=[' <=50K.',' >50K.'],\n",
    "                           value=['<=50K','>50K'],\n",
    "                           inplace=True)\n",
    "    print(df_test.income.value_counts())\n",
    "\n",
    "# ========================================================\n",
    "# define objects that can encode each variable as integer    \n",
    "encoders = dict() # save each encoder in dictionary\n",
    "categorical_headers = ['workclass','education','marital_status',\n",
    "                       'occupation','relationship','race','sex','country']\n",
    "\n",
    "# train all encoders (special case the target 'income')\n",
    "for col in categorical_headers+['income']:\n",
    "    df_train[col] = df_train[col].str.strip()\n",
    "    df_test[col] = df_test[col].str.strip()\n",
    "    \n",
    "    if col==\"income\":\n",
    "        # special case the target, just replace the column\n",
    "        tmp = LabelEncoder()\n",
    "        df_train[col] = tmp.fit_transform(df_train[col])\n",
    "        df_test[col] = tmp.transform(df_test[col])\n",
    "    else:\n",
    "        # integer encode strings that are features\n",
    "        encoders[col] = LabelEncoder() # save the encoder\n",
    "        df_train[col+'_int'] = encoders[col].fit_transform(df_train[col])\n",
    "        df_test[col+'_int'] = encoders[col].transform(df_test[col])\n",
    "\n",
    "# ========================================================\n",
    "# scale the numeric, continuous variables\n",
    "numeric_headers = [\"age\", \"cap_gain\", \"cap_loss\",\"work_hrs_weekly\"]\n",
    "\n",
    "ss = StandardScaler()\n",
    "df_train[numeric_headers] = ss.fit_transform(df_train[numeric_headers].values)\n",
    "df_test[numeric_headers] = ss.transform(df_test[numeric_headers].values)\n",
    "\n",
    "# Code above does the same as the following, for each column:\n",
    "# for col in numeric_headers:\n",
    "#     df_train[col] = df_train[col].astype(np.float)\n",
    "#     df_test[col] = df_test[col].astype(np.float)\n",
    "    \n",
    "#     ss = StandardScaler()\n",
    "#     df_train[col] = ss.fit_transform(df_train[col].values.reshape(-1, 1))\n",
    "#     df_test[col] = ss.transform(df_test[col].values.reshape(-1, 1))\n",
    "    \n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2073f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start as simply as possible, without any feature preprocessing\n",
    "categorical_headers_ints = [x+'_int' for x in categorical_headers]\n",
    "\n",
    "# we will forego one-hot encoding right now and instead just use all inputs as-is\n",
    "#   this is just to get an example running in Keras (its not a good idea)\n",
    "feature_columns = categorical_headers_ints+numeric_headers\n",
    "\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "print(f\"We will use the following {len(feature_columns)} features:\")\n",
    "pp.pprint(feature_columns)\n",
    "\n",
    "# combine the features into a single large matrix\n",
    "X_train = df_train[feature_columns].to_numpy()\n",
    "X_test = df_test[feature_columns].to_numpy()\n",
    "\n",
    "y_train = df_train['income'].values.astype(np.int32)\n",
    "y_test = df_test['income'].values.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec460e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have the following lists now of data that we can use with our dataframes:\n",
    "print(\"Numeric Headers:\")\n",
    "pp.pprint(numeric_headers) # normalized numeric data\n",
    "print(\"\\nCategorical String Headers:\")\n",
    "pp.pprint(categorical_headers) # string data\n",
    "print(\"\\nCategorical Headers, Encoded as Integer:\")\n",
    "pp.pprint(categorical_headers_ints) # string data encoded as an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ec1df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's define the architecture for a multi-layer network\n",
    "\n",
    "# First, lets setup the input size\n",
    "num_features = X_train.shape[1]\n",
    "input_tensor = Input(shape=(num_features,))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "# Dense means a fully connected layer, with 10 hidden neurons and a bias term\n",
    "x = Dense(units=10, activation='relu')(input_tensor)\n",
    "x = Dense(units=5, activation='tanh')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(inputs=input_tensor, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25909564",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4cb77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# you will need to install pydot properly on your machine to get this running\n",
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba7eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=50, verbose=1)\n",
    "\n",
    "# now lets see how well the model performed\n",
    "\n",
    "yhat_proba = model.predict(X_test) # sigmoid output probabilities\n",
    "yhat = np.round(yhat_proba) # round to get binary class\n",
    "\n",
    "print(mt.confusion_matrix(y_test,yhat))\n",
    "print(mt.classification_report(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c82ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# start by getting only the categorical variables\n",
    "# these matrices are all integers\n",
    "X_train = df_train[categorical_headers_ints].to_numpy() \n",
    "X_test = df_test[categorical_headers_ints].to_numpy() \n",
    "\n",
    "embed_branches = []\n",
    "all_branch_outputs = [] # this is where we will keep track of output of each branch\n",
    "\n",
    "# feed in the entire matrix of categircal variables\n",
    "input_branch = Input(shape=(X_train.shape[1],), \n",
    "                     dtype='int64', \n",
    "                     name='categorical')\n",
    "\n",
    "# for each categorical variable\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "    \n",
    "    # what the maximum integer value for this variable?\n",
    "    # which is the same as the number of categories\n",
    "    # these lines of code do the same thing\n",
    "    N = max(df_train[col].max(),df_test[col].max())+1\n",
    "    N = len(encoders[categorical_headers[idx]].classes_)\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_branch, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs in list to concatenate later\n",
    "    all_branch_outputs.append(x)\n",
    "    \n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "final_branch = concatenate(all_branch_outputs, name='concat_1')\n",
    "final_branch = Dense(units=1,\n",
    "                     activation='sigmoid', \n",
    "                     name='combined')(final_branch)\n",
    "\n",
    "model = Model(inputs=input_branch, outputs=final_branch)\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac8d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=50, verbose=1)\n",
    "\n",
    "\n",
    "yhat_proba = model.predict(X_test)\n",
    "yhat = np.round(yhat_proba)\n",
    "print(mt.confusion_matrix(y_test,yhat))\n",
    "print(mt.classification_report(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57625331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save categorical features\n",
    "X_train_cat = df_train[categorical_headers_ints].to_numpy() \n",
    "X_test_cat = df_test[categorical_headers_ints].to_numpy() \n",
    "\n",
    "# and save off the numeric features\n",
    "X_train_num =  df_train[numeric_headers].to_numpy()\n",
    "X_test_num = df_test[numeric_headers].to_numpy()\n",
    "\n",
    "all_branch_outputs = [] # this is where we will keep track of output of each branch\n",
    "\n",
    "# -----Like previous example, use embeddings from the integer data -----\n",
    "input_cat = Input(shape=(X_train_cat.shape[1],), dtype='int64', name='categorical')\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "    \n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = max(df_train[col].max(),df_test[col].max())+1\n",
    "    N = len(encoders[categorical_headers[idx]].classes_)\n",
    "    N_reduced = int(np.sqrt(N))\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_cat, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=N_reduced, \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    all_branch_outputs.append(x)\n",
    "    \n",
    "# -----HERE IS THE ADDING OF AN INPUT USING NUMERIC DATA-----\n",
    "# create dense input branch for numeric\n",
    "inputs_num = Input(shape=(X_train_num.shape[1],), name='numeric')\n",
    "x_dense = Dense(units=9, activation='relu', name='num_1')(inputs_num)\n",
    "    \n",
    "all_branch_outputs.append(x_dense)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "final_branch = concatenate(all_branch_outputs, name='concat_1')\n",
    "final_branch = Dense(units=10, activation='relu', name='combined_1')(final_branch)\n",
    "final_branch = Dense(units=1, activation='sigmoid', name='combined_2')(final_branch)\n",
    "\n",
    "model = Model(inputs=[input_cat,inputs_num], outputs=final_branch)\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f8b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit([ X_train_cat, X_train_num ], # inputs for each branch are a list\n",
    "          y_train, \n",
    "          epochs=10, \n",
    "          batch_size=50, \n",
    "          verbose=1)\n",
    "\n",
    "yhat = model.predict([X_test_cat,\n",
    "                      X_test_num]) # each branch has an input\n",
    "\n",
    "yhat = np.round(yhat)\n",
    "print(mt.confusion_matrix(y_test,yhat))\n",
    "print(mt.classification_report(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox for looking at different categorical variables\n",
    "for col in categorical_headers:\n",
    "    vals = df_train[col].unique()\n",
    "    print(col,'has', len(vals), 'unique values:')\n",
    "    print(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a4cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a quick example of crossing some columns\n",
    "\n",
    "cols_list = ['marital_status','workclass','relationship']\n",
    "\n",
    "# 1. create crossed labels by string join operation\n",
    "X_crossed_train = df_train[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "X_crossed_test = df_test[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "# combine together for training\n",
    "all_vals = np.hstack((X_crossed_train.to_numpy(),  X_crossed_test.to_numpy()))\n",
    "print(np.unique(all_vals))\n",
    "    \n",
    "# 2. encode as integers, stacking all possibilities\n",
    "enc = LabelEncoder()\n",
    "enc.fit(all_vals)\n",
    "\n",
    "encoded_vals_train = enc.transform(X_crossed_train)\n",
    "encoded_vals_test  = enc.transform(X_crossed_test)\n",
    "\n",
    "print(np.min(encoded_vals_train), np.max(encoded_vals_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679f61ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create some different crossed values\n",
    "# possible crossing options:\n",
    "#   'workclass','education','marital_status',\n",
    "#   'occupation','relationship','race',\n",
    "#   'sex','country'\n",
    "\n",
    "# choose these as a class, what makes sense??\n",
    "cross_columns = [#['race','sex','education','occupation'],\n",
    "                 ['race','sex', 'marital_status','occupation'],\n",
    "                 ['workclass','occupation'],\n",
    "                 ['workclass','occupation','education'],\n",
    "                 #['occupation','race','education'],\n",
    "                 ['education','country']\n",
    "                ]\n",
    "\n",
    "# cross each set of columns in the list above\n",
    "cross_col_df_names = []\n",
    "for cols_list in cross_columns:\n",
    "    # encode as ints for the embedding\n",
    "    enc = LabelEncoder()\n",
    "    \n",
    "    # 1. create crossed labels by join operation\n",
    "    X_crossed_train = df_train[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "    X_crossed_test = df_test[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "    \n",
    "    # get a nice name for this new crossed column\n",
    "    cross_col_name = '_'.join(cols_list)\n",
    "    \n",
    "    # 2. encode as integers, stacking all possibilities\n",
    "    enc.fit(np.hstack((X_crossed_train.to_numpy(),  X_crossed_test.to_numpy())))\n",
    "    \n",
    "    # 3. Save into dataframe with new name\n",
    "    df_train[cross_col_name] = enc.transform(X_crossed_train)\n",
    "    df_test[cross_col_name] = enc.transform(X_crossed_test)\n",
    "    \n",
    "    # Save the encoder used here for later:\n",
    "    encoders[cross_col_name] = enc\n",
    "    \n",
    "    # keep track of the new names of the crossed columns\n",
    "    cross_col_df_names.append(cross_col_name) \n",
    "    \n",
    "cross_col_df_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283a53c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model only using crossed values\n",
    "# get crossed columns\n",
    "X_train_crossed = df_train[cross_col_df_names].to_numpy()\n",
    "X_test_crossed = df_test[cross_col_df_names].to_numpy()\n",
    "\n",
    "crossed_outputs = [] # this is where we will keep track of output of each branch\n",
    "\n",
    "input_crossed = Input(shape=(X_train_crossed.shape[1],), dtype='int64', name='categorical')\n",
    "for idx,col in enumerate(cross_col_df_names):\n",
    "    \n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = max(df_train[col].max(),df_test[col].max())+1\n",
    "    N = len(encoders[col].classes_)\n",
    "    N_reduced = int(np.sqrt(N))\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_crossed, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=N_reduced, \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "    \n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch = concatenate(crossed_outputs, name='concat_1')\n",
    "wide_branch = Dense(units=1,activation='sigmoid', name='combined')(wide_branch)\n",
    "\n",
    "model = Model(inputs=input_crossed, outputs=wide_branch)\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_crossed,\n",
    "        y_train, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a2c271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bonus: we can keep training, picking up where we left of (no reinitialization)\n",
    "model.fit(X_train_crossed,\n",
    "        y_train, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795edbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.round(model.predict(X_test_crossed))\n",
    "print(mt.confusion_matrix(y_test,yhat))\n",
    "print(mt.classification_report(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f7a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will need to install pydot properly on your machine to get this running\n",
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff66eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get crossed columns\n",
    "X_train_crossed = df_train[cross_col_df_names].to_numpy()\n",
    "X_test_crossed = df_test[cross_col_df_names].to_numpy()\n",
    "\n",
    "# save categorical features\n",
    "X_train_cat = df_train[categorical_headers_ints].to_numpy() \n",
    "X_test_cat = df_test[categorical_headers_ints].to_numpy() \n",
    "\n",
    "# and save off the numeric features\n",
    "X_train_num =  df_train[numeric_headers].to_numpy()\n",
    "X_test_num = df_test[numeric_headers].to_numpy()\n",
    "\n",
    "\n",
    "# we need to create separate lists for each branch\n",
    "crossed_outputs = []\n",
    "\n",
    "# CROSSED DATA INPUT\n",
    "input_crossed = Input(shape=(X_train_crossed.shape[1],), dtype='int64', name='wide_inputs')\n",
    "for idx,col in enumerate(cross_col_df_names):\n",
    "    \n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = max(df_train[col].max(),df_test[col].max())+1\n",
    "    N = len(encoders[col].classes_)\n",
    "    N_reduced = int(np.sqrt(N))\n",
    "    \n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_crossed, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=N_reduced, \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "    \n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch = concatenate(crossed_outputs, name='wide_concat')\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs = []\n",
    "\n",
    "# CATEGORICAL DATA INPUT\n",
    "input_cat = Input(shape=(X_train_cat.shape[1],), dtype='int64', name='categorical_input')\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "    \n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = max(df_train[col].max(),df_test[col].max())+1\n",
    "    N_reduced = int(np.sqrt(N))\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_cat, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=N_reduced, \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    all_deep_branch_outputs.append(x)\n",
    "    \n",
    "# NUMERIC DATA INPUT\n",
    "# create dense input branch for numeric\n",
    "input_num = Input(shape=(X_train_num.shape[1],), name='numeric')\n",
    "x_dense = Dense(units=20, activation='relu',name='num_1')(input_num)\n",
    "    \n",
    "all_deep_branch_outputs.append(x_dense)\n",
    "\n",
    "\n",
    "# merge the deep branches together\n",
    "deep_branch = concatenate(all_deep_branch_outputs,name='concat_embeds')\n",
    "deep_branch = Dense(units=50,activation='relu', name='deep1')(deep_branch)\n",
    "deep_branch = Dense(units=25,activation='relu', name='deep2')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu', name='deep3')(deep_branch)\n",
    "    \n",
    "# merge the deep and wide branch\n",
    "final_branch = concatenate([wide_branch, deep_branch],\n",
    "                           name='concat_deep_wide')\n",
    "final_branch = Dense(units=1,activation='sigmoid',\n",
    "                     name='combined')(final_branch)\n",
    "\n",
    "model = Model(inputs=[input_crossed,input_cat,input_num], \n",
    "              outputs=final_branch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ebfeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will need to install pydot properly on your machine to get this running\n",
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2082d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.compile(optimizer='adagrad',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# lets also add the history variable to see how we are doing\n",
    "# and lets add a validation set to keep track of our progress\n",
    "history = model.fit([X_train_crossed,X_train_cat,X_train_num],\n",
    "                    y_train, \n",
    "                    epochs=15, \n",
    "                    batch_size=32, \n",
    "                    verbose=1, \n",
    "                    validation_data = ([X_test_crossed,X_test_cat,X_test_num],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a134d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([X_train_crossed,X_train_cat,X_train_num],\n",
    "                    y_train, \n",
    "                    epochs=50, \n",
    "                    batch_size=32, \n",
    "                    verbose=0, \n",
    "                    validation_data = ([X_test_crossed,X_test_cat,X_test_num],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.round(model.predict([X_test_crossed,X_test_cat,X_test_num]))\n",
    "print(mt.confusion_matrix(y_test,yhat))\n",
    "print(mt.classification_report(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365316cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "\n",
    "plt.ylabel('Accuracy %')\n",
    "plt.title('Training')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Validation')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.ylabel('Training Loss')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epochs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
